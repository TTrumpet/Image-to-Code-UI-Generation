{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be17c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e519f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<?, ?it/s]\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:37<00:00,  9.43s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing image_139_48.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to llava_next_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "# --------------------------\n",
    "# Settings\n",
    "# --------------------------\n",
    "MODEL_NAME = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "IMAGE_DIR = \"inference_images\"   # folder with wireframe images\n",
    "OUTPUT_FILE = \"llava_next_results.csv\"\n",
    "\n",
    "PROMPT_TEXT = \"\"\"You are analyzing a webpage wireframe. \n",
    "Give a thorough, section-by-section description of the layout. \n",
    "Include details such as:\n",
    "- The structure and arrangement of elements\n",
    "- Exact labels of buttons, menus, and CTAs\n",
    "- Headings, subheadings, and text content\n",
    "- Relationships between sections (e.g., grid layout, columns, rows)\n",
    "- Notes about emphasis, grouping, and visual hierarchy\n",
    "\n",
    "Do not summarize. Instead, expand with full descriptive sentences that would help a designer or developer rebuild this wireframe faithfully.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------\n",
    "# Load model & processor\n",
    "# --------------------------\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_NAME)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Helper function\n",
    "# --------------------------\n",
    "def infer(image_path, prompt_text=PROMPT_TEXT):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # conversation: text first, then image\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=512)\n",
    "    return processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# --------------------------\n",
    "# Run over all images\n",
    "# --------------------------\n",
    "results = []\n",
    "for fname in os.listdir(IMAGE_DIR):\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "        try:\n",
    "            desc = infer(path)\n",
    "            results.append({\"image\": fname, \"description\": desc})\n",
    "        except Exception as e:\n",
    "            results.append({\"image\": fname, \"description\": f\"ERROR: {e}\"})\n",
    "\n",
    "# --------------------------\n",
    "# Save results\n",
    "# --------------------------\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a154930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:09<00:00,  1.87s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 964.65it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 828.75it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Processing image_139_48.png...\n",
      "üîç Processing image_5578_23.png...\n",
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen2vl_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from qwen_vl_utils import process_vision_info  # comes with the model repo\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "IMAGE_DIR = \"inference_images\"\n",
    "OUTPUT_FILE = \"qwen2vl_results.csv\"\n",
    "\n",
    "PROMPT = (\n",
    "    \"You are analyzing a webpage wireframe. \"\n",
    "    \"Give a very detailed section-by-section description, including:\\n\"\n",
    "    \"- Header and navigation items\\n\"\n",
    "    \"- Hero section text and call-to-action\\n\"\n",
    "    \"- Content blocks with titles, descriptions, and buttons\\n\"\n",
    "    \"- Footer elements\\n\\n\"\n",
    "    \"Be exhaustive, using full sentences that would help a designer rebuild the layout.\"\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",   # best balance for accuracy\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # or torch.float16 if your GPU doesn‚Äôt support bf16\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Load processor & model\n",
    "# --------------------------\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def infer(image_path, prompt=PROMPT):\n",
    "    # Prepare chat message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    # Chat template\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # Vision inputs\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Final processor call\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "# --------------------------\n",
    "# Process folder\n",
    "# --------------------------\n",
    "results = []\n",
    "for fname in os.listdir(IMAGE_DIR):\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "        try:\n",
    "            desc = infer(path)\n",
    "            results.append({\"image\": fname, \"description\": desc})\n",
    "        except Exception as e:\n",
    "            results.append({\"image\": fname, \"description\": f\"ERROR: {e}\"})\n",
    "\n",
    "# --------------------------\n",
    "# Save results\n",
    "# --------------------------\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbbca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\llava\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:40<00:00, 10.08s/it]\n",
      "c:\\Users\\fa076154\\.conda\\envs\\llava\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fa076154\\.cache\\huggingface\\hub\\models--OpenGVLab--InternVL3_5-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing image_139_48.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing image_5578_23.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to internvl35_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# ---- Simple loader (resize only) ----\n",
    "def load_image_simple(image_file, input_size=448):\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image = image.resize((input_size, input_size))\n",
    "    pixel_values = torch.tensor(np.array(image)).permute(2,0,1).unsqueeze(0)  # [1,3,H,W]\n",
    "    return pixel_values\n",
    "\n",
    "# ---- Model Setup ----\n",
    "MODEL_NAME = \"OpenGVLab/InternVL3_5-8B\"\n",
    "IMAGE_DIR = \"inference_images\"\n",
    "OUTPUT_FILE = \"internvl35_results.csv\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16   # use float16 if bf16 not supported\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "PROMPT = (\n",
    "    \"You are analyzing a webpage wireframe. \"\n",
    "    \"Give a very detailed section-by-section description, including:\\n\"\n",
    "    \"- Header and navigation items\\n\"\n",
    "    \"- Hero section text and call-to-action\\n\"\n",
    "    \"- Content blocks with titles, descriptions, and buttons\\n\"\n",
    "    \"- Footer elements\\n\\n\"\n",
    "    \"Be exhaustive, using full sentences that would help a designer rebuild the layout.\"\n",
    ")\n",
    "\n",
    "# ---- Inference ----\n",
    "def infer(image_path, prompt=PROMPT):\n",
    "    pixel_values = load_image_simple(image_path).to(model.device, dtype=torch.float16)\n",
    "    response = model.chat(tokenizer, pixel_values, prompt, generation_config)\n",
    "    return response\n",
    "\n",
    "# ---- Batch Run ----\n",
    "results = []\n",
    "for fname in os.listdir(IMAGE_DIR):\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "        try:\n",
    "            desc = infer(path)\n",
    "            results.append({\"image\": fname, \"description\": desc})\n",
    "        except Exception as e:\n",
    "            results.append({\"image\": fname, \"description\": f\"ERROR: {e}\"})\n",
    "\n",
    "pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0daf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\llava\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:46<00:00,  9.35s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Processing image_139_48.png...\n",
      "üîç Processing image_5578_23.png...\n",
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen2vl_results.csv\n",
      "üìÇ HTML files saved in: outputs_html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from qwen_vl_utils import process_vision_info  # from the model repo\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "IMAGE_DIR = \"inference_images\"\n",
    "OUTPUT_CSV = \"qwen2vl_results.csv\"\n",
    "HTML_OUT_DIR = \"outputs_html\"\n",
    "MAX_NEW_TOKENS_DESC = 1024\n",
    "MAX_NEW_TOKENS_HTML = 1400\n",
    "\n",
    "PROMPT_DESC = (\n",
    "    \"You are analyzing a webpage wireframe. \"\n",
    "    \"Give a very detailed section-by-section description, including:\\n\"\n",
    "    \"- Header and navigation items\\n\"\n",
    "    \"- Hero section text and call-to-action\\n\"\n",
    "    \"- Content blocks with titles, descriptions, and buttons\\n\"\n",
    "    \"- Footer elements\\n\\n\"\n",
    "    \"Be exhaustive, using full sentences that would help a designer rebuild the layout.\"\n",
    ")\n",
    "\n",
    "PROMPT_HTML_IMAGE_ONLY = (\n",
    "    \"You are an expert front-end developer. Given ONLY the wireframe image, \"\n",
    "    \"generate a complete, minimal, responsive HTML5 page that approximates the layout.\\n\\n\"\n",
    "    \"Requirements:\\n\"\n",
    "    \"- Use semantic HTML tags (header, nav, main, section, footer, etc.).\\n\"\n",
    "    \"- Include a minimal <style> block (no external CSS/JS). Keep CSS concise.\\n\"\n",
    "    \"- Use placeholder text for headlines, paragraphs, buttons, links.\\n\"\n",
    "    \"- Structure should reflect the wireframe‚Äôs hierarchy (header, hero, content blocks, footer, etc.).\\n\"\n",
    "    \"- Avoid any JavaScript.\\n\\n\"\n",
    "    \"Output ONLY the HTML document.\"\n",
    ")\n",
    "\n",
    "PROMPT_HTML_WITH_DESC = (\n",
    "    \"You are an expert front-end developer. Use BOTH the wireframe image AND the provided textual description \"\n",
    "    \"to generate a complete, minimal, responsive HTML5 page that matches the layout.\\n\\n\"\n",
    "    \"Requirements:\\n\"\n",
    "    \"- Follow the described hierarchy (header/nav, hero, content blocks, footer) as closely as possible.\\n\"\n",
    "    \"- Use semantic HTML tags and a small <style> block (no external CSS/JS).\\n\"\n",
    "    \"- Use the description‚Äôs section names and CTA labels as placeholders.\\n\"\n",
    "    \"- Avoid any JavaScript.\\n\\n\"\n",
    "    \"Output ONLY the HTML document.\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Model loading (CUDA-aware)\n",
    "# --------------------------\n",
    "def load_model_and_processor():\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    quant = None\n",
    "    dtype = \"auto\"\n",
    "    device_map = \"auto\" if has_cuda else \"cpu\"\n",
    "\n",
    "    if has_cuda:\n",
    "        # 4-bit quant when GPU is available\n",
    "        quant = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_model_and_processor()\n",
    "os.makedirs(HTML_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Core generation helpers\n",
    "# --------------------------\n",
    "def _gen_from_messages(messages, max_new_tokens=1024):\n",
    "    \"\"\"\n",
    "    Generic chat invoke for Qwen2-VL using apply_chat_template + process_vision_info.\n",
    "    Returns string output.\n",
    "    \"\"\"\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,          # deterministic for consistency\n",
    "            temperature=None,\n",
    "        )\n",
    "\n",
    "    # Trim the prompt tokens\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output = processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return output[0]\n",
    "\n",
    "def describe_wireframe(image_path, prompt=PROMPT_DESC):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "    return _gen_from_messages(messages, max_new_tokens=MAX_NEW_TOKENS_DESC)\n",
    "\n",
    "def html_from_image_only(image_path, prompt=PROMPT_HTML_IMAGE_ONLY):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "    return _gen_from_messages(messages, max_new_tokens=MAX_NEW_TOKENS_HTML)\n",
    "\n",
    "def html_from_image_and_desc(image_path, description, prompt=PROMPT_HTML_WITH_DESC):\n",
    "    # We pass the image and then include the description as additional context.\n",
    "    desc_block = (\n",
    "        \"Here is the textual description you MUST follow where possible:\\n\\n\"\n",
    "        f\"{description}\\n\\n\"\n",
    "        \"‚Äî End of description ‚Äî\"\n",
    "    )\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\",  \"text\": desc_block},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "    return _gen_from_messages(messages, max_new_tokens=MAX_NEW_TOKENS_HTML)\n",
    "\n",
    "# --------------------------\n",
    "# Batch over folder\n",
    "# --------------------------\n",
    "def main():\n",
    "    results = []\n",
    "    image_dir = Path(IMAGE_DIR)\n",
    "    for fname in sorted(os.listdir(image_dir)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            continue\n",
    "\n",
    "        path = str(image_dir / fname)\n",
    "        stem = Path(fname).stem\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "\n",
    "        row = {\"image\": fname, \"description\": \"\", \"html_from_image\": \"\", \"html_from_image_plus_desc\": \"\"}\n",
    "\n",
    "        # 1) Description\n",
    "        try:\n",
    "            desc = describe_wireframe(path)\n",
    "            row[\"description\"] = desc\n",
    "        except Exception as e:\n",
    "            row[\"description\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        # 2) HTML from image only\n",
    "        try:\n",
    "            html_img = html_from_image_only(path)\n",
    "            row[\"html_from_image\"] = html_img\n",
    "            # Save HTML file\n",
    "            out_file_img = os.path.join(HTML_OUT_DIR, f\"{stem}__img_only.html\")\n",
    "            with open(out_file_img, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_img)\n",
    "        except Exception as e:\n",
    "            row[\"html_from_image\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        # 3) HTML from image + description (only if description succeeded)\n",
    "        try:\n",
    "            if row[\"description\"] and not row[\"description\"].startswith(\"ERROR:\"):\n",
    "                html_plus = html_from_image_and_desc(path, row[\"description\"])\n",
    "                row[\"html_from_image_plus_desc\"] = html_plus\n",
    "                out_file_plus = os.path.join(HTML_OUT_DIR, f\"{stem}__img_plus_desc.html\")\n",
    "                with open(out_file_plus, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html_plus)\n",
    "            else:\n",
    "                row[\"html_from_image_plus_desc\"] = \"SKIPPED: No valid description to condition on.\"\n",
    "        except Exception as e:\n",
    "            row[\"html_from_image_plus_desc\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\\nüìÇ HTML files saved in: {HTML_OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29ce72",
   "metadata": {},
   "source": [
    "Chain of thought reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d83bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:43<00:00,  8.62s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 750.59it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Processing image_139_48.png...\n",
      "üîç Processing image_5578_23.png...\n",
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen2vl_results_cot_html.csv\n",
      "üìÇ Outputs in: outputs_html_cot\n"
     ]
    }
   ],
   "source": [
    "# script1_cot_html.py\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "IMAGE_DIR = \"inference_images\"\n",
    "OUTPUT_CSV = \"qwen2vl_results_cot_html.csv\"\n",
    "HTML_OUT_DIR = \"outputs_html_cot\"\n",
    "MAX_NEW_TOKENS = 1700\n",
    "\n",
    "PROMPT_HTML_IMAGE_ONLY = (\n",
    "    \"You are an expert front-end developer. Given ONLY the wireframe image, produce exactly two sections:\\n\\n\"\n",
    "    \"1) <analysis>  Provide your step-by-step reasoning about how the layout maps to semantic HTML. \"\n",
    "    \"Identify header/nav items, hero structure, content blocks, and footer; explain layout choices (grid/flex), \"\n",
    "    \"responsiveness, and any assumptions you must make. Keep this as clear prose or short bullet points. </analysis>\\n\\n\"\n",
    "    \"2) <final>  Output a complete, minimal, responsive HTML5 document that reflects the layout. \"\n",
    "    \"Include <!doctype html>, <html>, <head> with <meta charset> and <meta name=\\\"viewport\\\">, \"\n",
    "    \"and a minimal <style> block in <head> (no external CSS/JS). Use semantic tags (header, nav, main, section, footer), \"\n",
    "    \"placeholder text for headings, paragraphs, and buttons, and accessible attributes where sensible. \"\n",
    "    \"Avoid all JavaScript.  </final>\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Do not repeat or paraphrase these instructions in your output.\\n\"\n",
    "    \"- Start your answer with <analysis> and end with </final>.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Load model\n",
    "# --------------------------\n",
    "def load_model_and_processor():\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    quant = None\n",
    "    dtype = \"auto\"\n",
    "    device_map = \"auto\" if has_cuda else \"cpu\"\n",
    "\n",
    "    if has_cuda:\n",
    "        quant = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_model_and_processor()\n",
    "os.makedirs(HTML_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def extract_final(output_text: str):\n",
    "    match = re.search(r\"<final>(.*?)</final>\", output_text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return output_text.strip()\n",
    "\n",
    "def html_from_image_only(image_path):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_HTML_IMAGE_ONLY},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image_path], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "\n",
    "    # Trim only if model actually generated something\n",
    "    outputs = []\n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids):\n",
    "        if len(out_ids) > len(in_ids):\n",
    "            trimmed = out_ids[len(in_ids):]\n",
    "        else:\n",
    "            trimmed = out_ids  # fallback: return everything\n",
    "        outputs.append(trimmed)\n",
    "\n",
    "    output_texts = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_texts[0]\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "def main():\n",
    "    results = []\n",
    "    for fname in sorted(os.listdir(IMAGE_DIR)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        stem = Path(fname).stem\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "\n",
    "        row = {\"image\": fname, \"html_raw\": \"\", \"html_final\": \"\"}\n",
    "\n",
    "        try:\n",
    "            raw_output = html_from_image_only(path)\n",
    "            final_output = extract_final(raw_output)\n",
    "\n",
    "            row[\"html_raw\"] = raw_output\n",
    "            row[\"html_final\"] = final_output\n",
    "\n",
    "            # Save final HTML separately\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__img_only.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_output)\n",
    "\n",
    "            # Save raw CoT reasoning + HTML\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__img_only_raw.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(raw_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"html_raw\"] = f\"ERROR: {e}\"\n",
    "            row[\"html_final\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\\nüìÇ Outputs in: {HTML_OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb8b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comparing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Comparing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Comparing image_139_48.png...\n",
      "üîç Comparing image_5578_23.png...\n",
      "üîç Comparing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen2vl_comparisons.csv\n"
     ]
    }
   ],
   "source": [
    "# compare_wireframes.py\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume model & processor are already loaded:\n",
    "#   model = Qwen2VLForConditionalGeneration.from_pretrained(...).to(\"cuda\")\n",
    "#   processor = AutoProcessor.from_pretrained(...)\n",
    "\n",
    "GT_DIR = \"inference_images\"\n",
    "GEN_DIR = \"direct_desc_gen\"\n",
    "OUTPUT_CSV = \"qwen2vl_comparisons.csv\"\n",
    "MAX_NEW_TOKENS = 800\n",
    "\n",
    "PROMPT_COMPARE = (\n",
    "    \"You are a strict UI/UX evaluator.\\n\"\n",
    "    \"Compare the first image (ground-truth wireframe) with the second image (generated wireframe).\\n\"\n",
    "    \"Do the following:\\n\"\n",
    "    \"1. Provide a detailed section-by-section comparison (header, hero, content blocks, footer).\\n\"\n",
    "    \"2. Explain key differences in structure, layout, and visual hierarchy.\\n\"\n",
    "    \"3. Assign a similarity score from 1 to 5 (1 = very different, 5 = nearly identical). Be very critical and you can assign scores in decimals.\\n\\n\"\n",
    "    \"Output format:\\n\"\n",
    "    \"<analysis> ... reasoning here ... </analysis>\\n\"\n",
    "    \"<score>n</score>\\n\"\n",
    ")\n",
    "\n",
    "def extract_score(text: str):\n",
    "    match = re.search(r\"<score>([1-5])</score>\", text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def compare_images(gt_path, gen_path):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": gt_path},\n",
    "            {\"type\": \"image\", \"image\": gen_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_COMPARE},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[gt_path, gen_path], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return output_text, extract_score(output_text)\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    gt_files = sorted([f for f in os.listdir(GT_DIR) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
    "\n",
    "    for fname in gt_files:\n",
    "        gt_path = os.path.join(GT_DIR, fname)\n",
    "        gen_path = os.path.join(GEN_DIR, fname)\n",
    "        if not os.path.exists(gen_path):\n",
    "            print(f\"‚ö†Ô∏è Skipping {fname} (no generated version found)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üîç Comparing {fname}...\")\n",
    "        try:\n",
    "            explanation, score = compare_images(gt_path, gen_path)\n",
    "            results.append({\"image\": fname, \"explanation\": explanation, \"score\": score})\n",
    "        except Exception as e:\n",
    "            results.append({\"image\": fname, \"explanation\": f\"ERROR: {e}\", \"score\": None})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062b403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "178a5c96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da4738c",
   "metadata": {},
   "source": [
    "Qwen 2.5 vl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81533e",
   "metadata": {},
   "source": [
    "COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249eef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:46<00:00,  9.27s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Processing image_139_48.png...\n",
      "üîç Processing image_5578_23.png...\n",
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen25vl_results_cot_html.csv\n",
      "üìÇ Outputs in: outputs_html_cot_25\n"
     ]
    }
   ],
   "source": [
    "# script1_cot_html.py\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "IMAGE_DIR = \"inference_images\"   # <-- updated directory name\n",
    "OUTPUT_CSV = \"qwen25vl_results_cot_html.csv\"\n",
    "HTML_OUT_DIR = \"outputs_html_cot_25\"   # <-- updated output dir\n",
    "MAX_NEW_TOKENS = 1700\n",
    "\n",
    "PROMPT_HTML_IMAGE_ONLY = (\n",
    "    \"You are an expert front-end developer. Given ONLY the wireframe image, produce exactly two sections:\\n\\n\"\n",
    "    \"1) <analysis>  Provide your step-by-step reasoning about how the layout maps to semantic HTML. \"\n",
    "    \"Identify header/nav items, hero structure, content blocks, and footer; explain layout choices (grid/flex), \"\n",
    "    \"responsiveness, and any assumptions you must make. Keep this as clear prose or short bullet points. </analysis>\\n\\n\"\n",
    "    \"2) <final>  Output a complete, minimal, responsive HTML5 document that reflects the layout. \"\n",
    "    \"Include <!doctype html>, <html>, <head> with <meta charset> and <meta name=\\\"viewport\\\">, \"\n",
    "    \"and a minimal <style> block in <head> (no external CSS/JS). Use semantic tags (header, nav, main, section, footer), \"\n",
    "    \"placeholder text for headings, paragraphs, and buttons, and accessible attributes where sensible. \"\n",
    "    \"Avoid all JavaScript.  </final>\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Do not repeat or paraphrase these instructions in your output.\\n\"\n",
    "    \"- Start your answer with <analysis> and end with </final>.\\n\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Load model\n",
    "# --------------------------\n",
    "def load_model_and_processor():\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    quant = None\n",
    "    dtype = \"auto\"\n",
    "    device_map = \"auto\" if has_cuda else \"cpu\"\n",
    "\n",
    "    if has_cuda:\n",
    "        quant = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_model_and_processor()\n",
    "os.makedirs(HTML_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def extract_final(output_text: str):\n",
    "    match = re.search(r\"<final>(.*?)</final>\", output_text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return output_text.strip()\n",
    "\n",
    "def html_from_image_only(image_path):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_HTML_IMAGE_ONLY},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image_path], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "\n",
    "    # Trim only if model actually generated something\n",
    "    outputs = []\n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids):\n",
    "        if len(out_ids) > len(in_ids):\n",
    "            trimmed = out_ids[len(in_ids):]\n",
    "        else:\n",
    "            trimmed = out_ids  # fallback\n",
    "        outputs.append(trimmed)\n",
    "\n",
    "    output_texts = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_texts[0]\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "def main():\n",
    "    results = []\n",
    "    for fname in sorted(os.listdir(IMAGE_DIR)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        stem = Path(fname).stem\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "\n",
    "        row = {\"image\": fname, \"html_raw\": \"\", \"html_final\": \"\"}\n",
    "\n",
    "        try:\n",
    "            raw_output = html_from_image_only(path)\n",
    "            final_output = extract_final(raw_output)\n",
    "\n",
    "            row[\"html_raw\"] = raw_output\n",
    "            row[\"html_final\"] = final_output\n",
    "\n",
    "            # Save only clean HTML inside <final> tags\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__img_only.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_output)\n",
    "\n",
    "            # Save full reasoning + final separately\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__img_only_raw.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(raw_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"html_raw\"] = f\"ERROR: {e}\"\n",
    "            row[\"html_final\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\\nüìÇ Outputs in: {HTML_OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f30bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.20s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Processing image_139_48.png...\n",
      "üîç Processing image_5578_23.png...\n",
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen25vl_results_direct_html.csv\n",
      "üìÇ HTML outputs in: outputs_html_direct_25\n"
     ]
    }
   ],
   "source": [
    "# script1_direct_html.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "IMAGE_DIR = \"inference_images\"   # same directory as before\n",
    "OUTPUT_CSV = \"qwen25vl_results_direct_html.csv\"\n",
    "HTML_OUT_DIR = \"outputs_html_direct_25\"\n",
    "MAX_NEW_TOKENS = 1700\n",
    "\n",
    "PROMPT_HTML_IMAGE_ONLY = (\n",
    "    \"You are an expert front-end developer. Given ONLY the wireframe image, \"\n",
    "    \"generate a complete, minimal, responsive HTML5 page that approximates the layout.\\n\\n\"\n",
    "    \"Requirements:\\n\"\n",
    "    \"- Use semantic HTML tags (header, nav, main, section, footer, etc.).\\n\"\n",
    "    \"- Include a minimal <style> block (no external CSS/JS). Keep CSS concise.\\n\"\n",
    "    \"- Use placeholder text for headlines, paragraphs, buttons, links.\\n\"\n",
    "    \"- Structure should reflect the wireframe‚Äôs hierarchy (header, hero, content blocks, footer, etc.).\\n\"\n",
    "    \"- Avoid any JavaScript.\\n\\n\"\n",
    "    \"Output ONLY the HTML document.\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Load model\n",
    "# --------------------------\n",
    "def load_model_and_processor():\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    quant = None\n",
    "    dtype = \"auto\"\n",
    "    device_map = \"auto\" if has_cuda else \"cpu\"\n",
    "\n",
    "    if has_cuda:\n",
    "        quant = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_model_and_processor()\n",
    "os.makedirs(HTML_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def html_from_image_only(image_path):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_HTML_IMAGE_ONLY},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image_path], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "\n",
    "    # Trim only if model actually generated something\n",
    "    outputs = []\n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids):\n",
    "        if len(out_ids) > len(in_ids):\n",
    "            trimmed = out_ids[len(in_ids):]\n",
    "        else:\n",
    "            trimmed = out_ids\n",
    "        outputs.append(trimmed)\n",
    "\n",
    "    output_texts = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_texts[0]\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "def main():\n",
    "    results = []\n",
    "    for fname in sorted(os.listdir(IMAGE_DIR)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        stem = Path(fname).stem\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "\n",
    "        row = {\"image\": fname, \"html_output\": \"\"}\n",
    "\n",
    "        try:\n",
    "            html_output = html_from_image_only(path)\n",
    "            row[\"html_output\"] = html_output\n",
    "\n",
    "            # Save HTML directly\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__direct.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"html_output\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\\nüìÇ HTML outputs in: {HTML_OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53704407",
   "metadata": {},
   "source": [
    "with descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44787244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.20s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Processing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Processing image_139_48.png...\n",
      "üîç Processing image_5578_23.png...\n",
      "üîç Processing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen25vl_results_desc_html.csv\n",
      "üìÇ Outputs in: outputs_html_desc_25\n"
     ]
    }
   ],
   "source": [
    "# script2_desc_html.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "IMAGE_DIR = \"inference_images\"\n",
    "OUTPUT_CSV = \"qwen25vl_results_desc_html.csv\"\n",
    "HTML_OUT_DIR = \"outputs_html_desc_25\"\n",
    "MAX_NEW_TOKENS_DESC = 1200\n",
    "MAX_NEW_TOKENS_HTML = 1700\n",
    "\n",
    "PROMPT_DESC = (\n",
    "    \"You are analyzing a webpage wireframe. \"\n",
    "    \"Give a very detailed section-by-section description, including:\\n\"\n",
    "    \"- Header and navigation items\\n\"\n",
    "    \"- Hero section text and call-to-action\\n\"\n",
    "    \"- Content blocks with titles, descriptions, and buttons\\n\"\n",
    "    \"- Footer elements\\n\\n\"\n",
    "    \"Be exhaustive, using full sentences that would help a designer rebuild the layout.\"\n",
    ")\n",
    "\n",
    "PROMPT_HTML_WITH_DESC = (\n",
    "    \"You are an expert front-end developer. Use BOTH the wireframe image AND the provided textual description \"\n",
    "    \"to generate a complete, minimal, responsive HTML5 page that matches the layout.\\n\\n\"\n",
    "    \"Requirements:\\n\"\n",
    "    \"- Follow the described hierarchy (header/nav, hero, content blocks, footer) as closely as possible.\\n\"\n",
    "    \"- Use semantic HTML tags and a small <style> block (no external CSS/JS).\\n\"\n",
    "    \"- Use the description‚Äôs section names and CTA labels as placeholders.\\n\"\n",
    "    \"- Avoid any JavaScript.\\n\\n\"\n",
    "    \"Output ONLY the HTML document.\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Load model\n",
    "# --------------------------\n",
    "def load_model_and_processor():\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    quant = None\n",
    "    dtype = \"auto\"\n",
    "    device_map = \"auto\" if has_cuda else \"cpu\"\n",
    "\n",
    "    if has_cuda:\n",
    "        quant = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_model_and_processor()\n",
    "os.makedirs(HTML_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def generate_text(messages, max_new_tokens=1024):\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[m[\"content\"][0][\"image\"] for m in messages if \"image\" in str(m)], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "\n",
    "    outputs = []\n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids):\n",
    "        trimmed = out_ids[len(in_ids):] if len(out_ids) > len(in_ids) else out_ids\n",
    "        outputs.append(trimmed)\n",
    "\n",
    "    return processor.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "def describe_wireframe(image_path):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_DESC},\n",
    "        ],\n",
    "    }]\n",
    "    return generate_text(messages, max_new_tokens=MAX_NEW_TOKENS_DESC)\n",
    "\n",
    "def html_from_image_and_desc(image_path, description):\n",
    "    desc_block = \"Here is the textual description you MUST follow:\\n\\n\" + description\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": desc_block},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_HTML_WITH_DESC},\n",
    "        ],\n",
    "    }]\n",
    "    return generate_text(messages, max_new_tokens=MAX_NEW_TOKENS_HTML)\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "def main():\n",
    "    results = []\n",
    "    for fname in sorted(os.listdir(IMAGE_DIR)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(IMAGE_DIR, fname)\n",
    "        stem = Path(fname).stem\n",
    "        print(f\"üîç Processing {fname}...\")\n",
    "\n",
    "        row = {\"image\": fname, \"description\": \"\", \"html_output\": \"\"}\n",
    "\n",
    "        try:\n",
    "            # Step 1: Get description\n",
    "            desc = describe_wireframe(path)\n",
    "            row[\"description\"] = desc\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__desc.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(desc)\n",
    "\n",
    "            # Step 2: Generate HTML with description\n",
    "            html_output = html_from_image_and_desc(path, desc)\n",
    "            row[\"html_output\"] = html_output\n",
    "            with open(os.path.join(HTML_OUT_DIR, f\"{stem}__html_from_image_plus_desc.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"description\"] = f\"ERROR: {e}\"\n",
    "            row[\"html_output\"] = f\"ERROR: {e}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\\nüìÇ Outputs in: {HTML_OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b9eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comparing 3AD8509F-A870-4A05-BF80-909BED5EED6A.png...\n",
      "üîç Comparing 3C0A4C08-F7BF-42DD-A72D-4A379B66B529.png...\n",
      "üîç Comparing image_139_48.png...\n",
      "üîç Comparing image_5578_23.png...\n",
      "üîç Comparing image_79_37.png...\n",
      "\n",
      "‚úÖ Done! Results saved to qwen25vl_comparisons_desc.csv\n"
     ]
    }
   ],
   "source": [
    "# compare_wireframes_25.py\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "GT_DIR = \"inference_images\"        # Ground-truth wireframes\n",
    "GEN_DIR = \"desc_gen_25\"        # Generated wireframes\n",
    "OUTPUT_CSV = \"qwen25vl_comparisons_desc.csv\"\n",
    "MAX_NEW_TOKENS = 800\n",
    "\n",
    "PROMPT_COMPARE = (\n",
    "    \"You are a strict UI/UX evaluator.\\n\"\n",
    "    \"Compare the first image (ground-truth wireframe) with the second image (generated wireframe).\\n\"\n",
    "    \"Do the following:\\n\"\n",
    "    \"1. Provide a detailed section-by-section comparison (header, hero, content blocks, footer).\\n\"\n",
    "    \"2. Explain key differences in structure, layout, and visual hierarchy.\\n\"\n",
    "    \"3. Assign a similarity score from 1 to 5 (1 = very different, 5 = nearly identical). \"\n",
    "    \"You may use decimal values like 3.2 or 4.7. Be very critical.\\n\\n\"\n",
    "    \"Output format:\\n\"\n",
    "    \"<analysis> ... reasoning here ... </analysis>\\n\"\n",
    "    \"<score>n</score>\\n\"\n",
    ")\n",
    "\n",
    "def extract_score(text: str):\n",
    "    # Extract integer or decimal score between <score> tags\n",
    "    match = re.search(r\"<score>([0-5](?:\\.\\d+)?)</score>\", text)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def compare_images(gt_path, gen_path, model, processor):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": gt_path},\n",
    "            {\"type\": \"image\", \"image\": gen_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_COMPARE},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[gt_path, gen_path], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return output_text, extract_score(output_text)\n",
    "\n",
    "def main(model, processor):\n",
    "    results = []\n",
    "    gt_files = sorted([f for f in os.listdir(GT_DIR) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\"))])\n",
    "\n",
    "    for fname in gt_files:\n",
    "        gt_path = os.path.join(GT_DIR, fname)\n",
    "        gen_path = os.path.join(GEN_DIR, fname)\n",
    "        if not os.path.exists(gen_path):\n",
    "            print(f\"‚ö†Ô∏è Skipping {fname} (no generated version found)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üîç Comparing {fname}...\")\n",
    "        try:\n",
    "            explanation, score = compare_images(gt_path, gen_path, model, processor)\n",
    "            results.append({\"image\": fname, \"explanation\": explanation, \"score\": score})\n",
    "        except Exception as e:\n",
    "            results.append({\"image\": fname, \"explanation\": f\"ERROR: {e}\", \"score\": None})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Done! Results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "# Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    main(model, processor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3c1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dccd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c21be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2e151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6fd7bc",
   "metadata": {},
   "source": [
    "Heirarchy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f04026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.16s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting hierarchy for image_0_1.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting hierarchy for image_0_10.png...\n",
      "üîç Extracting hierarchy for image_0_11.png...\n",
      "üîç Extracting hierarchy for image_0_12.png...\n",
      "üîç Extracting hierarchy for image_0_13.png...\n",
      "üîç Extracting hierarchy for image_0_15.png...\n",
      "üîç Extracting hierarchy for image_0_16.png...\n",
      "üîç Extracting hierarchy for image_0_18.png...\n",
      "üîç Extracting hierarchy for image_0_19.png...\n",
      "üîç Extracting hierarchy for image_0_2.png...\n",
      "üîç Extracting hierarchy for image_0_20.png...\n",
      "üîç Extracting hierarchy for image_0_21.png...\n",
      "üîç Extracting hierarchy for image_0_22.png...\n",
      "üîç Extracting hierarchy for image_0_23.png...\n",
      "üîç Extracting hierarchy for image_0_24.png...\n",
      "üîç Extracting hierarchy for image_0_25.png...\n",
      "üîç Extracting hierarchy for image_0_26.png...\n",
      "üîç Extracting hierarchy for image_0_27.png...\n",
      "üîç Extracting hierarchy for image_0_28.png...\n",
      "üîç Extracting hierarchy for image_0_29.png...\n",
      "üîç Extracting hierarchy for image_0_3.png...\n",
      "üîç Extracting hierarchy for image_0_30.png...\n",
      "üîç Extracting hierarchy for image_0_31.png...\n",
      "üîç Extracting hierarchy for image_0_32.png...\n",
      "üîç Extracting hierarchy for image_0_33.png...\n",
      "üîç Extracting hierarchy for image_0_34.png...\n",
      "üîç Extracting hierarchy for image_0_35.png...\n",
      "üîç Extracting hierarchy for image_0_37.png...\n",
      "üîç Extracting hierarchy for image_0_38.png...\n",
      "üîç Extracting hierarchy for image_0_39.png...\n",
      "üîç Extracting hierarchy for image_0_4.png...\n",
      "üîç Extracting hierarchy for image_0_40.png...\n",
      "üîç Extracting hierarchy for image_0_42.png...\n",
      "üîç Extracting hierarchy for image_0_43.png...\n",
      "üîç Extracting hierarchy for image_0_44.png...\n",
      "üîç Extracting hierarchy for image_0_45.png...\n",
      "üîç Extracting hierarchy for image_0_46.png...\n",
      "üîç Extracting hierarchy for image_0_47.png...\n",
      "üîç Extracting hierarchy for image_0_48.png...\n",
      "üîç Extracting hierarchy for image_0_49.png...\n",
      "üîç Extracting hierarchy for image_0_5.png...\n",
      "üîç Extracting hierarchy for image_0_50.png...\n",
      "üîç Extracting hierarchy for image_0_51.png...\n",
      "üîç Extracting hierarchy for image_0_52.png...\n",
      "üîç Extracting hierarchy for image_0_53.png...\n",
      "üîç Extracting hierarchy for image_0_54.png...\n",
      "üîç Extracting hierarchy for image_0_55.png...\n",
      "üîç Extracting hierarchy for image_0_56.png...\n",
      "üîç Extracting hierarchy for image_0_57.png...\n",
      "üîç Extracting hierarchy for image_0_58.png...\n",
      "üîç Extracting hierarchy for image_0_59.png...\n",
      "üîç Extracting hierarchy for image_0_6.png...\n",
      "üîç Extracting hierarchy for image_0_60.png...\n",
      "üîç Extracting hierarchy for image_0_61.png...\n",
      "üîç Extracting hierarchy for image_0_62.png...\n",
      "üîç Extracting hierarchy for image_0_63.png...\n",
      "üîç Extracting hierarchy for image_0_64.png...\n",
      "üîç Extracting hierarchy for image_0_65.png...\n",
      "üîç Extracting hierarchy for image_0_66.png...\n",
      "üîç Extracting hierarchy for image_0_67.png...\n",
      "üîç Extracting hierarchy for image_0_68.png...\n",
      "üîç Extracting hierarchy for image_0_69.png...\n",
      "üîç Extracting hierarchy for image_0_7.png...\n",
      "üîç Extracting hierarchy for image_0_70.png...\n",
      "üîç Extracting hierarchy for image_0_71.png...\n",
      "üîç Extracting hierarchy for image_0_8.png...\n",
      "üîç Extracting hierarchy for image_0_9.png...\n",
      "üîç Extracting hierarchy for image_1_1.png...\n",
      "üîç Extracting hierarchy for image_1_10.png...\n",
      "üîç Extracting hierarchy for image_1_11.png...\n",
      "üîç Extracting hierarchy for image_1_12.png...\n",
      "üîç Extracting hierarchy for image_1_13.png...\n",
      "üîç Extracting hierarchy for image_1_14.png...\n",
      "üîç Extracting hierarchy for image_1_15.png...\n",
      "üîç Extracting hierarchy for image_1_16.png...\n",
      "üîç Extracting hierarchy for image_1_17.png...\n",
      "üîç Extracting hierarchy for image_1_18.png...\n",
      "üîç Extracting hierarchy for image_1_19.png...\n",
      "üîç Extracting hierarchy for image_1_2.png...\n",
      "üîç Extracting hierarchy for image_1_21.png...\n",
      "üîç Extracting hierarchy for image_1_22.png...\n",
      "üîç Extracting hierarchy for image_1_23.png...\n",
      "üîç Extracting hierarchy for image_1_24.png...\n",
      "üîç Extracting hierarchy for image_1_25.png...\n",
      "üîç Extracting hierarchy for image_1_26.png...\n",
      "üîç Extracting hierarchy for image_1_27.png...\n",
      "üîç Extracting hierarchy for image_1_28.png...\n",
      "üîç Extracting hierarchy for image_1_29.png...\n",
      "üîç Extracting hierarchy for image_1_3.png...\n",
      "üîç Extracting hierarchy for image_1_30.png...\n",
      "üîç Extracting hierarchy for image_1_31.png...\n",
      "üîç Extracting hierarchy for image_1_32.png...\n",
      "üîç Extracting hierarchy for image_1_33.png...\n",
      "üîç Extracting hierarchy for image_1_34.png...\n",
      "üîç Extracting hierarchy for image_1_35.png...\n",
      "üîç Extracting hierarchy for image_1_36.png...\n",
      "üîç Extracting hierarchy for image_1_37.png...\n",
      "üîç Extracting hierarchy for image_1_39.png...\n",
      "üîç Extracting hierarchy for image_1_4.png...\n",
      "üîç Extracting hierarchy for image_1_40.png...\n",
      "üîç Extracting hierarchy for image_1_41.png...\n",
      "üîç Extracting hierarchy for image_1_42.png...\n",
      "üîç Extracting hierarchy for image_1_43.png...\n",
      "üîç Extracting hierarchy for image_1_44.png...\n",
      "üîç Extracting hierarchy for image_1_45.png...\n",
      "üîç Extracting hierarchy for image_1_46.png...\n",
      "üîç Extracting hierarchy for image_1_47.png...\n",
      "üîç Extracting hierarchy for image_1_48.png...\n",
      "üîç Extracting hierarchy for image_1_49.png...\n",
      "üîç Extracting hierarchy for image_1_5.png...\n",
      "üîç Extracting hierarchy for image_1_50.png...\n",
      "üîç Extracting hierarchy for image_1_51.png...\n",
      "üîç Extracting hierarchy for image_1_52.png...\n",
      "üîç Extracting hierarchy for image_1_53.png...\n",
      "üîç Extracting hierarchy for image_1_55.png...\n",
      "üîç Extracting hierarchy for image_1_56.png...\n",
      "üîç Extracting hierarchy for image_1_57.png...\n",
      "üîç Extracting hierarchy for image_1_58.png...\n",
      "üîç Extracting hierarchy for image_1_59.png...\n",
      "üîç Extracting hierarchy for image_1_6.png...\n",
      "üîç Extracting hierarchy for image_1_60.png...\n",
      "üîç Extracting hierarchy for image_1_61.png...\n",
      "üîç Extracting hierarchy for image_1_62.png...\n",
      "üîç Extracting hierarchy for image_1_63.png...\n",
      "üîç Extracting hierarchy for image_1_64.png...\n",
      "üîç Extracting hierarchy for image_1_66.png...\n",
      "üîç Extracting hierarchy for image_1_67.png...\n",
      "üîç Extracting hierarchy for image_1_68.png...\n",
      "üîç Extracting hierarchy for image_1_69.png...\n",
      "üîç Extracting hierarchy for image_1_7.png...\n",
      "üîç Extracting hierarchy for image_1_70.png...\n",
      "üîç Extracting hierarchy for image_1_71.png...\n",
      "üîç Extracting hierarchy for image_1_8.png...\n",
      "üîç Extracting hierarchy for image_1_9.png...\n",
      "üîç Extracting hierarchy for image_2_1.png...\n",
      "üîç Extracting hierarchy for image_2_10.png...\n",
      "üîç Extracting hierarchy for image_2_11.png...\n",
      "üîç Extracting hierarchy for image_2_12.png...\n",
      "üîç Extracting hierarchy for image_2_13.png...\n",
      "üîç Extracting hierarchy for image_2_14.png...\n",
      "üîç Extracting hierarchy for image_2_15.png...\n",
      "üîç Extracting hierarchy for image_2_16.png...\n",
      "üîç Extracting hierarchy for image_2_17.png...\n",
      "üîç Extracting hierarchy for image_2_18.png...\n",
      "üîç Extracting hierarchy for image_2_19.png...\n",
      "üîç Extracting hierarchy for image_2_2.png...\n",
      "üîç Extracting hierarchy for image_2_20.png...\n",
      "üîç Extracting hierarchy for image_2_22.png...\n",
      "üîç Extracting hierarchy for image_2_23.png...\n",
      "üîç Extracting hierarchy for image_2_24.png...\n",
      "üîç Extracting hierarchy for image_2_25.png...\n",
      "üîç Extracting hierarchy for image_2_26.png...\n",
      "üîç Extracting hierarchy for image_2_27.png...\n",
      "üîç Extracting hierarchy for image_2_28.png...\n",
      "üîç Extracting hierarchy for image_2_29.png...\n",
      "üîç Extracting hierarchy for image_2_3.png...\n",
      "üîç Extracting hierarchy for image_2_30.png...\n",
      "üîç Extracting hierarchy for image_2_31.png...\n",
      "üîç Extracting hierarchy for image_2_32.png...\n",
      "üîç Extracting hierarchy for image_2_33.png...\n",
      "üîç Extracting hierarchy for image_2_34.png...\n",
      "üîç Extracting hierarchy for image_2_35.png...\n",
      "üîç Extracting hierarchy for image_2_36.png...\n",
      "üîç Extracting hierarchy for image_2_37.png...\n",
      "üîç Extracting hierarchy for image_2_38.png...\n",
      "üîç Extracting hierarchy for image_2_39.png...\n",
      "üîç Extracting hierarchy for image_2_4.png...\n",
      "üîç Extracting hierarchy for image_2_40.png...\n",
      "üîç Extracting hierarchy for image_2_41.png...\n",
      "üîç Extracting hierarchy for image_2_42.png...\n",
      "üîç Extracting hierarchy for image_2_43.png...\n",
      "üîç Extracting hierarchy for image_2_44.png...\n",
      "üîç Extracting hierarchy for image_2_45.png...\n",
      "üîç Extracting hierarchy for image_2_46.png...\n",
      "üîç Extracting hierarchy for image_2_47.png...\n",
      "üîç Extracting hierarchy for image_2_48.png...\n",
      "üîç Extracting hierarchy for image_2_49.png...\n",
      "üîç Extracting hierarchy for image_2_5.png...\n",
      "üîç Extracting hierarchy for image_2_50.png...\n",
      "üîç Extracting hierarchy for image_2_51.png...\n",
      "üîç Extracting hierarchy for image_2_52.png...\n",
      "üîç Extracting hierarchy for image_2_53.png...\n",
      "üîç Extracting hierarchy for image_2_54.png...\n",
      "üîç Extracting hierarchy for image_2_55.png...\n",
      "üîç Extracting hierarchy for image_2_56.png...\n",
      "üîç Extracting hierarchy for image_2_57.png...\n",
      "üîç Extracting hierarchy for image_2_58.png...\n",
      "üîç Extracting hierarchy for image_2_59.png...\n",
      "üîç Extracting hierarchy for image_2_6.png...\n",
      "üîç Extracting hierarchy for image_2_60.png...\n",
      "üîç Extracting hierarchy for image_2_61.png...\n",
      "üîç Extracting hierarchy for image_2_62.png...\n",
      "üîç Extracting hierarchy for image_2_63.png...\n",
      "üîç Extracting hierarchy for image_2_64.png...\n",
      "üîç Extracting hierarchy for image_2_66.png...\n",
      "üîç Extracting hierarchy for image_2_67.png...\n",
      "üîç Extracting hierarchy for image_2_68.png...\n",
      "üîç Extracting hierarchy for image_2_69.png...\n",
      "üîç Extracting hierarchy for image_2_7.png...\n",
      "üîç Extracting hierarchy for image_2_70.png...\n",
      "üîç Extracting hierarchy for image_2_71.png...\n",
      "üîç Extracting hierarchy for image_2_8.png...\n",
      "üîç Extracting hierarchy for image_2_9.png...\n",
      "üîç Extracting hierarchy for image_3_1.png...\n",
      "üîç Extracting hierarchy for image_3_10.png...\n",
      "üîç Extracting hierarchy for image_3_11.png...\n",
      "üîç Extracting hierarchy for image_3_12.png...\n",
      "üîç Extracting hierarchy for image_3_13.png...\n",
      "üîç Extracting hierarchy for image_3_14.png...\n",
      "üîç Extracting hierarchy for image_3_15.png...\n",
      "üîç Extracting hierarchy for image_3_16.png...\n",
      "üîç Extracting hierarchy for image_3_17.png...\n",
      "üîç Extracting hierarchy for image_3_18.png...\n",
      "üîç Extracting hierarchy for image_3_19.png...\n",
      "üîç Extracting hierarchy for image_3_2.png...\n",
      "üîç Extracting hierarchy for image_3_20.png...\n",
      "üîç Extracting hierarchy for image_3_21.png...\n",
      "üîç Extracting hierarchy for image_3_22.png...\n",
      "üîç Extracting hierarchy for image_3_23.png...\n",
      "üîç Extracting hierarchy for image_3_25.png...\n",
      "üîç Extracting hierarchy for image_3_26.png...\n",
      "üîç Extracting hierarchy for image_3_27.png...\n",
      "üîç Extracting hierarchy for image_3_28.png...\n",
      "üîç Extracting hierarchy for image_3_29.png...\n",
      "üîç Extracting hierarchy for image_3_3.png...\n",
      "üîç Extracting hierarchy for image_3_30.png...\n",
      "üîç Extracting hierarchy for image_3_31.png...\n",
      "üîç Extracting hierarchy for image_3_33.png...\n",
      "üîç Extracting hierarchy for image_3_34.png...\n",
      "üîç Extracting hierarchy for image_3_35.png...\n",
      "üîç Extracting hierarchy for image_3_36.png...\n",
      "üîç Extracting hierarchy for image_3_37.png...\n",
      "üîç Extracting hierarchy for image_3_38.png...\n",
      "üîç Extracting hierarchy for image_3_39.png...\n",
      "üîç Extracting hierarchy for image_3_4.png...\n",
      "üîç Extracting hierarchy for image_3_40.png...\n",
      "üîç Extracting hierarchy for image_3_41.png...\n",
      "üîç Extracting hierarchy for image_3_42.png...\n",
      "üîç Extracting hierarchy for image_3_43.png...\n",
      "üîç Extracting hierarchy for image_3_44.png...\n",
      "üîç Extracting hierarchy for image_3_45.png...\n",
      "üîç Extracting hierarchy for image_3_46.png...\n",
      "üîç Extracting hierarchy for image_3_47.png...\n",
      "üîç Extracting hierarchy for image_3_48.png...\n",
      "üîç Extracting hierarchy for image_3_49.png...\n",
      "üîç Extracting hierarchy for image_3_5.png...\n",
      "üîç Extracting hierarchy for image_3_50.png...\n",
      "üîç Extracting hierarchy for image_3_51.png...\n",
      "üîç Extracting hierarchy for image_3_52.png...\n",
      "üîç Extracting hierarchy for image_3_53.png...\n",
      "üîç Extracting hierarchy for image_3_54.png...\n",
      "üîç Extracting hierarchy for image_3_55.png...\n",
      "üîç Extracting hierarchy for image_3_56.png...\n",
      "üîç Extracting hierarchy for image_3_57.png...\n",
      "üîç Extracting hierarchy for image_3_58.png...\n",
      "üîç Extracting hierarchy for image_3_59.png...\n",
      "üîç Extracting hierarchy for image_3_6.png...\n",
      "üîç Extracting hierarchy for image_3_60.png...\n",
      "üîç Extracting hierarchy for image_3_61.png...\n",
      "üîç Extracting hierarchy for image_3_62.png...\n",
      "üîç Extracting hierarchy for image_3_63.png...\n",
      "üîç Extracting hierarchy for image_3_64.png...\n",
      "üîç Extracting hierarchy for image_3_65.png...\n",
      "üîç Extracting hierarchy for image_3_66.png...\n",
      "üîç Extracting hierarchy for image_3_67.png...\n",
      "üîç Extracting hierarchy for image_3_68.png...\n",
      "üîç Extracting hierarchy for image_3_69.png...\n",
      "üîç Extracting hierarchy for image_3_7.png...\n",
      "üîç Extracting hierarchy for image_3_70.png...\n",
      "üîç Extracting hierarchy for image_3_71.png...\n",
      "üîç Extracting hierarchy for image_3_8.png...\n",
      "üîç Extracting hierarchy for image_3_9.png...\n",
      "üîç Extracting hierarchy for image_4_1.png...\n",
      "üîç Extracting hierarchy for image_4_10.png...\n",
      "üîç Extracting hierarchy for image_4_11.png...\n",
      "üîç Extracting hierarchy for image_4_12.png...\n",
      "üîç Extracting hierarchy for image_4_13.png...\n",
      "üîç Extracting hierarchy for image_4_14.png...\n",
      "üîç Extracting hierarchy for image_4_15.png...\n",
      "üîç Extracting hierarchy for image_4_16.png...\n",
      "üîç Extracting hierarchy for image_4_17.png...\n",
      "üîç Extracting hierarchy for image_4_18.png...\n",
      "üîç Extracting hierarchy for image_4_19.png...\n",
      "üîç Extracting hierarchy for image_4_2.png...\n",
      "üîç Extracting hierarchy for image_4_20.png...\n",
      "üîç Extracting hierarchy for image_4_21.png...\n",
      "üîç Extracting hierarchy for image_4_22.png...\n",
      "üîç Extracting hierarchy for image_4_23.png...\n",
      "üîç Extracting hierarchy for image_4_24.png...\n",
      "üîç Extracting hierarchy for image_4_25.png...\n",
      "üîç Extracting hierarchy for image_4_26.png...\n",
      "üîç Extracting hierarchy for image_4_27.png...\n",
      "üîç Extracting hierarchy for image_4_28.png...\n",
      "üîç Extracting hierarchy for image_4_29.png...\n",
      "üîç Extracting hierarchy for image_4_3.png...\n",
      "üîç Extracting hierarchy for image_4_30.png...\n",
      "üîç Extracting hierarchy for image_4_31.png...\n",
      "üîç Extracting hierarchy for image_4_32.png...\n",
      "üîç Extracting hierarchy for image_4_33.png...\n",
      "üîç Extracting hierarchy for image_4_34.png...\n",
      "üîç Extracting hierarchy for image_4_35.png...\n",
      "üîç Extracting hierarchy for image_4_36.png...\n",
      "üîç Extracting hierarchy for image_4_37.png...\n",
      "üîç Extracting hierarchy for image_4_38.png...\n",
      "üîç Extracting hierarchy for image_4_39.png...\n",
      "üîç Extracting hierarchy for image_4_4.png...\n",
      "üîç Extracting hierarchy for image_4_40.png...\n",
      "üîç Extracting hierarchy for image_4_41.png...\n",
      "üîç Extracting hierarchy for image_4_42.png...\n",
      "üîç Extracting hierarchy for image_4_43.png...\n",
      "üîç Extracting hierarchy for image_4_44.png...\n",
      "üîç Extracting hierarchy for image_4_45.png...\n",
      "üîç Extracting hierarchy for image_4_46.png...\n",
      "üîç Extracting hierarchy for image_4_48.png...\n",
      "üîç Extracting hierarchy for image_4_49.png...\n",
      "üîç Extracting hierarchy for image_4_5.png...\n",
      "üîç Extracting hierarchy for image_4_50.png...\n",
      "üîç Extracting hierarchy for image_4_51.png...\n",
      "üîç Extracting hierarchy for image_4_52.png...\n",
      "üîç Extracting hierarchy for image_4_53.png...\n",
      "üîç Extracting hierarchy for image_4_54.png...\n",
      "üîç Extracting hierarchy for image_4_55.png...\n",
      "üîç Extracting hierarchy for image_4_56.png...\n",
      "üîç Extracting hierarchy for image_4_57.png...\n",
      "üîç Extracting hierarchy for image_4_58.png...\n",
      "üîç Extracting hierarchy for image_4_59.png...\n",
      "üîç Extracting hierarchy for image_4_6.png...\n",
      "üîç Extracting hierarchy for image_4_60.png...\n",
      "üîç Extracting hierarchy for image_4_61.png...\n",
      "üîç Extracting hierarchy for image_4_62.png...\n",
      "üîç Extracting hierarchy for image_4_63.png...\n",
      "üîç Extracting hierarchy for image_4_64.png...\n",
      "üîç Extracting hierarchy for image_4_66.png...\n",
      "üîç Extracting hierarchy for image_4_67.png...\n",
      "üîç Extracting hierarchy for image_4_68.png...\n",
      "üîç Extracting hierarchy for image_4_69.png...\n",
      "üîç Extracting hierarchy for image_4_7.png...\n",
      "üîç Extracting hierarchy for image_4_70.png...\n",
      "üîç Extracting hierarchy for image_4_71.png...\n",
      "üîç Extracting hierarchy for image_4_8.png...\n",
      "üîç Extracting hierarchy for image_4_9.png...\n",
      "üîç Extracting hierarchy for image_5_1.png...\n",
      "üîç Extracting hierarchy for image_5_10.png...\n",
      "üîç Extracting hierarchy for image_5_11.png...\n",
      "üîç Extracting hierarchy for image_5_12.png...\n",
      "üîç Extracting hierarchy for image_5_13.png...\n",
      "üîç Extracting hierarchy for image_5_14.png...\n",
      "üîç Extracting hierarchy for image_5_15.png...\n",
      "üîç Extracting hierarchy for image_5_16.png...\n",
      "üîç Extracting hierarchy for image_5_17.png...\n",
      "üîç Extracting hierarchy for image_5_18.png...\n",
      "üîç Extracting hierarchy for image_5_19.png...\n",
      "üîç Extracting hierarchy for image_5_2.png...\n",
      "üîç Extracting hierarchy for image_5_20.png...\n",
      "üîç Extracting hierarchy for image_5_22.png...\n",
      "üîç Extracting hierarchy for image_5_23.png...\n",
      "üîç Extracting hierarchy for image_5_24.png...\n",
      "üîç Extracting hierarchy for image_5_25.png...\n",
      "üîç Extracting hierarchy for image_5_26.png...\n",
      "üîç Extracting hierarchy for image_5_27.png...\n",
      "üîç Extracting hierarchy for image_5_28.png...\n",
      "üîç Extracting hierarchy for image_5_29.png...\n",
      "üîç Extracting hierarchy for image_5_3.png...\n",
      "üîç Extracting hierarchy for image_5_30.png...\n",
      "üîç Extracting hierarchy for image_5_31.png...\n",
      "üîç Extracting hierarchy for image_5_32.png...\n",
      "üîç Extracting hierarchy for image_5_33.png...\n",
      "üîç Extracting hierarchy for image_5_34.png...\n",
      "üîç Extracting hierarchy for image_5_35.png...\n",
      "üîç Extracting hierarchy for image_5_36.png...\n",
      "üîç Extracting hierarchy for image_5_37.png...\n",
      "üîç Extracting hierarchy for image_5_38.png...\n",
      "üîç Extracting hierarchy for image_5_39.png...\n",
      "üîç Extracting hierarchy for image_5_4.png...\n",
      "üîç Extracting hierarchy for image_5_40.png...\n",
      "üîç Extracting hierarchy for image_5_41.png...\n",
      "üîç Extracting hierarchy for image_5_42.png...\n",
      "üîç Extracting hierarchy for image_5_43.png...\n",
      "üîç Extracting hierarchy for image_5_44.png...\n",
      "üîç Extracting hierarchy for image_5_45.png...\n",
      "üîç Extracting hierarchy for image_5_46.png...\n",
      "üîç Extracting hierarchy for image_5_47.png...\n",
      "üîç Extracting hierarchy for image_5_48.png...\n",
      "üîç Extracting hierarchy for image_5_49.png...\n",
      "üîç Extracting hierarchy for image_5_5.png...\n",
      "üîç Extracting hierarchy for image_5_50.png...\n",
      "üîç Extracting hierarchy for image_5_51.png...\n",
      "üîç Extracting hierarchy for image_5_52.png...\n",
      "üîç Extracting hierarchy for image_5_53.png...\n",
      "üîç Extracting hierarchy for image_5_54.png...\n",
      "üîç Extracting hierarchy for image_5_55.png...\n",
      "üîç Extracting hierarchy for image_5_56.png...\n",
      "üîç Extracting hierarchy for image_5_57.png...\n",
      "üîç Extracting hierarchy for image_5_58.png...\n",
      "üîç Extracting hierarchy for image_5_59.png...\n",
      "üîç Extracting hierarchy for image_5_6.png...\n",
      "üîç Extracting hierarchy for image_5_60.png...\n",
      "üîç Extracting hierarchy for image_5_61.png...\n",
      "üîç Extracting hierarchy for image_5_62.png...\n",
      "üîç Extracting hierarchy for image_5_63.png...\n",
      "üîç Extracting hierarchy for image_5_64.png...\n",
      "üîç Extracting hierarchy for image_5_65.png...\n",
      "üîç Extracting hierarchy for image_5_66.png...\n",
      "üîç Extracting hierarchy for image_5_67.png...\n",
      "üîç Extracting hierarchy for image_5_68.png...\n",
      "üîç Extracting hierarchy for image_5_69.png...\n",
      "üîç Extracting hierarchy for image_5_7.png...\n",
      "üîç Extracting hierarchy for image_5_70.png...\n",
      "üîç Extracting hierarchy for image_5_71.png...\n",
      "üîç Extracting hierarchy for image_5_8.png...\n",
      "üîç Extracting hierarchy for image_5_9.png...\n",
      "üîç Extracting hierarchy for image_6_1.png...\n",
      "üîç Extracting hierarchy for image_6_10.png...\n",
      "üîç Extracting hierarchy for image_6_11.png...\n",
      "üîç Extracting hierarchy for image_6_12.png...\n",
      "üîç Extracting hierarchy for image_6_13.png...\n",
      "üîç Extracting hierarchy for image_6_14.png...\n",
      "üîç Extracting hierarchy for image_6_15.png...\n",
      "üîç Extracting hierarchy for image_6_16.png...\n",
      "üîç Extracting hierarchy for image_6_17.png...\n",
      "üîç Extracting hierarchy for image_6_18.png...\n",
      "üîç Extracting hierarchy for image_6_19.png...\n",
      "üîç Extracting hierarchy for image_6_2.png...\n",
      "üîç Extracting hierarchy for image_6_20.png...\n",
      "üîç Extracting hierarchy for image_6_21.png...\n",
      "üîç Extracting hierarchy for image_6_22.png...\n",
      "üîç Extracting hierarchy for image_6_23.png...\n",
      "üîç Extracting hierarchy for image_6_25.png...\n",
      "üîç Extracting hierarchy for image_6_26.png...\n",
      "üîç Extracting hierarchy for image_6_27.png...\n",
      "üîç Extracting hierarchy for image_6_28.png...\n",
      "üîç Extracting hierarchy for image_6_29.png...\n",
      "üîç Extracting hierarchy for image_6_3.png...\n",
      "üîç Extracting hierarchy for image_6_30.png...\n",
      "üîç Extracting hierarchy for image_6_31.png...\n",
      "üîç Extracting hierarchy for image_6_32.png...\n",
      "üîç Extracting hierarchy for image_6_34.png...\n",
      "üîç Extracting hierarchy for image_6_35.png...\n",
      "üîç Extracting hierarchy for image_6_36.png...\n",
      "üîç Extracting hierarchy for image_6_37.png...\n",
      "üîç Extracting hierarchy for image_6_38.png...\n",
      "üîç Extracting hierarchy for image_6_39.png...\n",
      "üîç Extracting hierarchy for image_6_4.png...\n",
      "üîç Extracting hierarchy for image_6_40.png...\n",
      "üîç Extracting hierarchy for image_6_41.png...\n",
      "üîç Extracting hierarchy for image_6_42.png...\n",
      "üîç Extracting hierarchy for image_6_43.png...\n",
      "üîç Extracting hierarchy for image_6_44.png...\n",
      "üîç Extracting hierarchy for image_6_45.png...\n",
      "üîç Extracting hierarchy for image_6_46.png...\n",
      "üîç Extracting hierarchy for image_6_47.png...\n",
      "üîç Extracting hierarchy for image_6_48.png...\n",
      "üîç Extracting hierarchy for image_6_49.png...\n",
      "üîç Extracting hierarchy for image_6_5.png...\n",
      "üîç Extracting hierarchy for image_6_50.png...\n",
      "üîç Extracting hierarchy for image_6_51.png...\n",
      "üîç Extracting hierarchy for image_6_52.png...\n",
      "üîç Extracting hierarchy for image_6_53.png...\n",
      "üîç Extracting hierarchy for image_6_54.png...\n",
      "üîç Extracting hierarchy for image_6_55.png...\n",
      "üîç Extracting hierarchy for image_6_56.png...\n",
      "üîç Extracting hierarchy for image_6_57.png...\n",
      "üîç Extracting hierarchy for image_6_58.png...\n",
      "üîç Extracting hierarchy for image_6_59.png...\n",
      "üîç Extracting hierarchy for image_6_6.png...\n",
      "üîç Extracting hierarchy for image_6_60.png...\n",
      "üîç Extracting hierarchy for image_6_61.png...\n",
      "üîç Extracting hierarchy for image_6_62.png...\n",
      "üîç Extracting hierarchy for image_6_63.png...\n",
      "üîç Extracting hierarchy for image_6_64.png...\n",
      "üîç Extracting hierarchy for image_6_65.png...\n",
      "üîç Extracting hierarchy for image_6_66.png...\n",
      "üîç Extracting hierarchy for image_6_67.png...\n",
      "üîç Extracting hierarchy for image_6_68.png...\n",
      "üîç Extracting hierarchy for image_6_69.png...\n",
      "üîç Extracting hierarchy for image_6_7.png...\n",
      "üîç Extracting hierarchy for image_6_70.png...\n",
      "üîç Extracting hierarchy for image_6_71.png...\n",
      "üîç Extracting hierarchy for image_6_9.png...\n",
      "üîç Extracting hierarchy for image_7_1.png...\n",
      "üîç Extracting hierarchy for image_7_10.png...\n",
      "üîç Extracting hierarchy for image_7_11.png...\n",
      "üîç Extracting hierarchy for image_7_12.png...\n",
      "üîç Extracting hierarchy for image_7_13.png...\n",
      "üîç Extracting hierarchy for image_7_14.png...\n",
      "üîç Extracting hierarchy for image_7_15.png...\n",
      "üîç Extracting hierarchy for image_7_16.png...\n",
      "üîç Extracting hierarchy for image_7_17.png...\n",
      "üîç Extracting hierarchy for image_7_18.png...\n",
      "üîç Extracting hierarchy for image_7_19.png...\n",
      "üîç Extracting hierarchy for image_7_2.png...\n",
      "üîç Extracting hierarchy for image_7_20.png...\n",
      "üîç Extracting hierarchy for image_7_21.png...\n",
      "üîç Extracting hierarchy for image_7_22.png...\n",
      "üîç Extracting hierarchy for image_7_23.png...\n",
      "üîç Extracting hierarchy for image_7_24.png...\n",
      "üîç Extracting hierarchy for image_7_25.png...\n",
      "üîç Extracting hierarchy for image_7_26.png...\n",
      "üîç Extracting hierarchy for image_7_27.png...\n",
      "üîç Extracting hierarchy for image_7_28.png...\n",
      "üîç Extracting hierarchy for image_7_29.png...\n",
      "üîç Extracting hierarchy for image_7_3.png...\n",
      "üîç Extracting hierarchy for image_7_31.png...\n",
      "üîç Extracting hierarchy for image_7_32.png...\n",
      "üîç Extracting hierarchy for image_7_33.png...\n",
      "üîç Extracting hierarchy for image_7_34.png...\n",
      "üîç Extracting hierarchy for image_7_35.png...\n",
      "üîç Extracting hierarchy for image_7_36.png...\n",
      "üîç Extracting hierarchy for image_7_37.png...\n",
      "üîç Extracting hierarchy for image_7_38.png...\n",
      "üîç Extracting hierarchy for image_7_39.png...\n",
      "üîç Extracting hierarchy for image_7_4.png...\n",
      "üîç Extracting hierarchy for image_7_40.png...\n",
      "üîç Extracting hierarchy for image_7_41.png...\n",
      "üîç Extracting hierarchy for image_7_42.png...\n",
      "üîç Extracting hierarchy for image_7_43.png...\n",
      "üîç Extracting hierarchy for image_7_44.png...\n",
      "üîç Extracting hierarchy for image_7_45.png...\n",
      "üîç Extracting hierarchy for image_7_46.png...\n",
      "üîç Extracting hierarchy for image_7_47.png...\n",
      "üîç Extracting hierarchy for image_7_48.png...\n",
      "üîç Extracting hierarchy for image_7_49.png...\n",
      "üîç Extracting hierarchy for image_7_5.png...\n",
      "üîç Extracting hierarchy for image_7_50.png...\n",
      "üîç Extracting hierarchy for image_7_51.png...\n",
      "üîç Extracting hierarchy for image_7_52.png...\n",
      "üîç Extracting hierarchy for image_7_53.png...\n",
      "üîç Extracting hierarchy for image_7_54.png...\n",
      "üîç Extracting hierarchy for image_7_55.png...\n",
      "üîç Extracting hierarchy for image_7_56.png...\n",
      "üîç Extracting hierarchy for image_7_57.png...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Extraction complete! Results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìÇ JSONs in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_JSON_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     raw_json \u001b[38;5;241m=\u001b[39m \u001b[43mextract_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_output\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_json\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Attempt to parse JSON\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 95\u001b[0m, in \u001b[0;36mextract_hierarchy\u001b[1;34m(image_path, model, processor)\u001b[0m\n\u001b[0;32m     92\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39m[text], images\u001b[38;5;241m=\u001b[39m[image_path], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 95\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_NEW_TOKENS, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     97\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\generation\\utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2530\u001b[0m         inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2535\u001b[0m     )\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2540\u001b[0m         input_ids,\n\u001b[0;32m   2541\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2542\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2543\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2544\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2545\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2546\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2547\u001b[0m     )\n\u001b[0;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[0;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2552\u001b[0m         input_ids,\n\u001b[0;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2558\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\generation\\utils.py:2870\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2868\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2870\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2872\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   2873\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   2874\u001b[0m     outputs,\n\u001b[0;32m   2875\u001b[0m     model_kwargs,\n\u001b[0;32m   2876\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2877\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\utils\\generic.py:940\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 940\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    942\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:1485\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m   1480\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[0;32m   1481\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1483\u001b[0m )\n\u001b[1;32m-> 1485\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1486\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1487\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m   1488\u001b[0m     pixel_values_videos\u001b[38;5;241m=\u001b[39mpixel_values_videos,\n\u001b[0;32m   1489\u001b[0m     image_grid_thw\u001b[38;5;241m=\u001b[39mimage_grid_thw,\n\u001b[0;32m   1490\u001b[0m     video_grid_thw\u001b[38;5;241m=\u001b[39mvideo_grid_thw,\n\u001b[0;32m   1491\u001b[0m     second_per_grid_ts\u001b[38;5;241m=\u001b[39msecond_per_grid_ts,\n\u001b[0;32m   1492\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1493\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1494\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1495\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1496\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1497\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1498\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1499\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1500\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1501\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:1313\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[0m\n\u001b[0;32m   1310\u001b[0m         delta \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39mrepeat_interleave(batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m delta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1311\u001b[0m         position_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39mto(position_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1313\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[0;32m   1314\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1315\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1316\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1317\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1318\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1319\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1320\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1321\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1322\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1325\u001b[0m )\n\u001b[0;32m   1327\u001b[0m output \u001b[38;5;241m=\u001b[39m Qwen2_5_VLModelOutputWithPast(\n\u001b[0;32m   1328\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mlast_hidden_state,\n\u001b[0;32m   1329\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mpast_key_values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     rope_deltas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_deltas,\n\u001b[0;32m   1333\u001b[0m )\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:902\u001b[0m, in \u001b[0;36mQwen2_5_VLTextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    900\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 902\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    903\u001b[0m     hidden_states,\n\u001b[0;32m    904\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type],\n\u001b[0;32m    905\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mtext_position_ids,\n\u001b[0;32m    906\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    907\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    908\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    909\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    910\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    912\u001b[0m )\n\u001b[0;32m    914\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:750\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    748\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m    753\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    754\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    755\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    763\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:199\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    197\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    198\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 199\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "IMAGE_DIR =  r\"C:\\Users\\fa076154\\Desktop\\CAP6411\\Inference\\Web2Code_image\\WebSight_images_new\"#\"inference_images\"\n",
    "OUTPUT_JSON_DIR = \"outputs_hierarchy_DPO_json_25\"#\"outputs_hierarchy_json_25\"\n",
    "OUTPUT_CSV = \"qwen25vl_component_hierarchy_DPO.csv\"#\"qwen25vl_component_hierarchy.csv\"\n",
    "MAX_NEW_TOKENS = 2500\n",
    "\n",
    "\n",
    "PROMPT_HIERARCHY = (\n",
    "    \"You are an expert UI layout analyzer. \"\n",
    "    \"Analyze this wireframe and output a precise, hierarchical JSON structure that captures the full layout and nesting of components.\\n\\n\"\n",
    "    \"Representation rules:\\n\"\n",
    "    \"- Each component is an object with:\\n\"\n",
    "    \"  ‚Ä¢ 'type': the component name (e.g., page, header, nav, hero, section, row, column, button, image, card, footer)\\n\"\n",
    "    \"  ‚Ä¢ 'attributes': a dictionary of visual attributes such as color, position (top/left), size, alignment, and text content if visible.\\n\"\n",
    "    \"  ‚Ä¢ 'children': a list of components visually contained within that element.\\n\\n\"\n",
    "    \"Hierarchy and structure guidelines:\\n\"\n",
    "    \"- The root node must be the full page ('type': 'page').\\n\"\n",
    "    \"- Preserve the **visual and logical nesting** ‚Äî if elements appear inside a container, section, or div, they must be represented as its children.\\n\"\n",
    "    \"- Group horizontally aligned components together within a 'row' container.\\n\"\n",
    "    \"- Within each 'row', represent vertical stacking as separate 'column' components where appropriate.\\n\"\n",
    "    \"- Maintain left-to-right and top-to-bottom order strictly as seen in the layout.\\n\"\n",
    "    \"- Ensure sibling components appear in correct sequence and avoid flattening nested structures.\\n\\n\"\n",
    "    \"Output requirements:\\n\"\n",
    "    \"- Output only **valid JSON** ‚Äî no text or explanation outside the JSON.\\n\"\n",
    "    \"- The structure must be complete and balanced (all brackets closed).\\n\"\n",
    "    \"- Pay particular attention to the **arrangement and nesting of divs**, preserving all parent‚Äìchild relationships exactly as seen visually.\"\n",
    ")\n",
    "\n",
    "\n",
    "# PROMPT_HIERARCHY = (\n",
    "#     \"You are an expert UI layout analyzer. \"\n",
    "#     \"Analyze this wireframe and output all visible components in a hierarchical JSON structure.\\n\\n\"\n",
    "#     \"Each component should be represented as an object with:\\n\"\n",
    "#     \"- 'type': the component name (e.g., header, nav, hero, button, image, card, footer)\\n\"\n",
    "#     \"- 'attributes': a dictionary with attributes like color, position, size, alignment, and text content if visible\\n\"\n",
    "#     \"- 'children': a list of nested components inside it\\n\\n\"\n",
    "#     \"The root node should represent the full page as 'page'.\\n\"\n",
    "#     \"Follow the visual hierarchy (top to bottom, left to right). Output valid JSON only‚Äîno text outside the JSON.\"\n",
    "# )\n",
    "\n",
    "# --------------------------\n",
    "# Load Model\n",
    "# --------------------------\n",
    "def load_model_and_processor():\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    quant = None\n",
    "    dtype = \"auto\"\n",
    "    device_map = \"auto\" if has_cuda else \"cpu\"\n",
    "\n",
    "    if has_cuda:\n",
    "        quant = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Inference Function\n",
    "# --------------------------\n",
    "def extract_hierarchy(image_path, model, processor):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_HIERARCHY},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image_path], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return output_text.strip()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Main Function\n",
    "# --------------------------\n",
    "def main():\n",
    "    model, processor = load_model_and_processor()\n",
    "    os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    image_dir = Path(IMAGE_DIR)\n",
    "\n",
    "    for fname in sorted(os.listdir(image_dir)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            continue\n",
    "\n",
    "        img_path = str(image_dir / fname)\n",
    "        stem = Path(fname).stem\n",
    "        print(f\"üîç Extracting hierarchy for {fname}...\")\n",
    "\n",
    "        row = {\"image\": fname, \"raw_output\": \"\", \"status\": \"\"}\n",
    "\n",
    "        try:\n",
    "            raw_json = extract_hierarchy(img_path, model, processor)\n",
    "            row[\"raw_output\"] = raw_json\n",
    "\n",
    "            # Attempt to parse JSON\n",
    "            try:\n",
    "                parsed_json = json.loads(raw_json)\n",
    "                json_path = os.path.join(OUTPUT_JSON_DIR, f\"{stem}_hierarchy.json\")\n",
    "                with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(parsed_json, f, indent=2)\n",
    "                row[\"status\"] = \"parsed\"\n",
    "            except json.JSONDecodeError:\n",
    "                # Save raw text if JSON invalid\n",
    "                with open(os.path.join(OUTPUT_JSON_DIR, f\"{stem}_raw.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(raw_json)\n",
    "                row[\"status\"] = \"invalid_json\"\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"raw_output\"] = f\"ERROR: {e}\"\n",
    "            row[\"status\"] = \"error\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Extraction complete! Results saved to {OUTPUT_CSV}\\nüìÇ JSONs in: {OUTPUT_JSON_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb878d8",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c635a6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fa076154\\.conda\\envs\\camvid_sam2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.10s/it]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen 2.5-VL loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# --------------------------\n",
    "# Model Configuration\n",
    "# --------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Efficient 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16   # use bfloat16 if your GPU supports it\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Load Model & Processor\n",
    "# --------------------------\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"‚úÖ Qwen 2.5-VL loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d070c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 1_raw.txt with image 1.png...\n",
      "üîç Processing 2_raw.txt with image 2.png...\n",
      "üîç Processing 3_raw.txt with image 3.png...\n",
      "\n",
      "‚úÖ Verification done! HTMLs saved in outputs_html_verify_sample_25\n",
      "üìä Log saved to qwen25vl_verify_hierarchy_results_sample_from_rawtxt.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "RAW_HIERARCHY_DIR = \"outputs_hierarchy_sample_json_25\"     # where *_raw.txt files are\n",
    "IMAGE_DIR = \"sample\"#\"inference_images\"\n",
    "OUTPUT_HTML_DIR = \"outputs_html_verify_sample_25\"#\"outputs_html_verify_25\"\n",
    "OUTPUT_CSV = \"qwen25vl_verify_hierarchy_results_sample_from_rawtxt.csv\"#\"qwen25vl_verify_hierarchy_results_from_rawtxt.csv\"\n",
    "MAX_NEW_TOKENS = 2500\n",
    "\n",
    "PROMPT_HTML_FROM_JSON = (\n",
    "    \"You are an expert front-end developer. \"\n",
    "    \"Use BOTH the following hierarchical component JSON and the provided wireframe image \"\n",
    "    \"to generate a complete, minimal, responsive HTML5 layout.\\n\\n\"\n",
    "    \"Guidelines:\\n\"\n",
    "    \"- Each node's 'type' corresponds to an HTML section or element.\\n\"\n",
    "    \"- Use semantic HTML tags (header, nav, main, section, article, footer, etc.).\\n\"\n",
    "    \"- Use node 'attributes' to infer inline styles (colors, alignment, size).\\n\"\n",
    "    \"- Preserve the hierarchy: parent nodes contain their children in proper order.\\n\"\n",
    "    \"- Include a minimal <style> block in <head> but no external CSS or JavaScript.\\n\"\n",
    "    \"- Use placeholder text for headings, paragraphs, or buttons.\\n\"\n",
    "    \"- The visual layout should reflect the image as closely as possible.\\n\\n\"\n",
    "    \"Output ONLY valid HTML code, starting with <!doctype html>.\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Helper: Extract JSON from raw text\n",
    "# --------------------------\n",
    "def extract_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Extract JSON code from a text block that may contain roles and markdown fences.\n",
    "    \"\"\"\n",
    "    # 1Ô∏è‚É£ Look for ```json ... ``` block first\n",
    "    match = re.search(r\"```json(.*?)```\", text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1).strip()\n",
    "    else:\n",
    "        # 2Ô∏è‚É£ Fallback: look for first JSON-like braces\n",
    "        match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
    "        if not match:\n",
    "            print(\"‚ö†Ô∏è No JSON found in file.\")\n",
    "            return None\n",
    "        json_str = match.group(0)\n",
    "\n",
    "    # 3Ô∏è‚É£ Try to parse cleanly\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ö†Ô∏è JSON parse error: {e}\")\n",
    "        # 4Ô∏è‚É£ Try to repair common issues (truncated JSON)\n",
    "        if json_str.count(\"{\") > json_str.count(\"}\"):\n",
    "            json_str += \"}\" * (json_str.count(\"{\") - json_str.count(\"}\"))\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "# --------------------------\n",
    "# Core Function\n",
    "# --------------------------\n",
    "def generate_html_from_json_and_image(json_data, image_path, model, processor):\n",
    "    json_text = json.dumps(json_data, indent=2)\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": PROMPT_HTML_FROM_JSON},\n",
    "            {\"type\": \"text\", \"text\": f\"Here is the JSON:\\n\\n{json_text}\"}\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image_path],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            temperature=None\n",
    "        )\n",
    "\n",
    "    trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "def main(model, processor):\n",
    "    os.makedirs(OUTPUT_HTML_DIR, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    raw_files = sorted([f for f in os.listdir(RAW_HIERARCHY_DIR) if f.endswith(\"_raw.txt\")])\n",
    "\n",
    "    for fname in raw_files:\n",
    "        raw_path = os.path.join(RAW_HIERARCHY_DIR, fname)\n",
    "        stem = Path(fname).stem.replace(\"_raw\", \"\")\n",
    "        image_candidates = [os.path.join(IMAGE_DIR, f\"{stem}.png\"), os.path.join(IMAGE_DIR, f\"{stem}.jpg\")]\n",
    "        image_path = next((p for p in image_candidates if os.path.exists(p)), None)\n",
    "\n",
    "        if not image_path:\n",
    "            print(f\"‚ö†Ô∏è Skipping {fname} ‚Äî No matching image found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üîç Processing {fname} with image {Path(image_path).name}...\")\n",
    "\n",
    "        row = {\"raw_file\": fname, \"image_file\": Path(image_path).name, \"status\": \"\", \"html_file\": \"\", \"raw_output\": \"\"}\n",
    "\n",
    "        try:\n",
    "            with open(raw_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "\n",
    "            json_data = extract_json_from_text(raw_text)\n",
    "            if json_data is None:\n",
    "                row[\"status\"] = \"error: no valid JSON\"\n",
    "                results.append(row)\n",
    "                continue\n",
    "\n",
    "            html_output = generate_html_from_json_and_image(json_data, image_path, model, processor)\n",
    "            row[\"raw_output\"] = html_output\n",
    "\n",
    "            out_html_path = os.path.join(OUTPUT_HTML_DIR, f\"{stem}__verify.html\")\n",
    "            with open(out_html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_output)\n",
    "\n",
    "            row[\"html_file\"] = out_html_path\n",
    "            row[\"status\"] = \"success\"\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"status\"] = f\"error: {e}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Verification done! HTMLs saved in {OUTPUT_HTML_DIR}\")\n",
    "    print(f\"üìä Log saved to {OUTPUT_CSV}\")\n",
    "\n",
    "# --------------------------\n",
    "# Example usage\n",
    "# --------------------------\n",
    "main(model, processor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camvid_sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
